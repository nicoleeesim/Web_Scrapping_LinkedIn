{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping LinkedIn Job Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import beautiful soup libs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import selenium libs\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Expand Website using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './chromedriver_win32/chromedriver.exe'  \n",
    "url = 'https://www.linkedin.com/jobs/search?keywords=Data%20Analyst&location=Singapore&geoId=102454443&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "\n",
    "# incognito mode\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver = webdriver.Chrome(executable_path=path, options=chrome_options)  \n",
    "driver.get(url)\n",
    "\n",
    "# allow webpage to load\n",
    "time.sleep(5) \n",
    "\n",
    "# set waiting time before next scroll\n",
    "SCROLL_PAUSE_TIME = 1\n",
    "\n",
    "# Get scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# infinite scrolling\n",
    "while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for page to load\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "# A 'show more' button appears at random and breaks the loop. Automate button clicks and infinite scrolling\n",
    "while True:    \n",
    "        driver.find_element_by_class_name('infinite-scroller__show-more-button').click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve web content using beautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from html \n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract required data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = soup.find_all('div', class_='base-card base-card--link base-search-card base-search-card--link job-search-card')\n",
    "\n",
    "job_title_lst = []\n",
    "job_url_lst = []\n",
    "company_lst = []\n",
    "company_url_lst = []\n",
    "\n",
    "for each in jobs:\n",
    "    \n",
    "    # find job_title\n",
    "    job_title = each.find('h3', class_='base-search-card__title')\n",
    "    job_title = job_title.get_text().strip()\n",
    "    job_title_lst.append(job_title)\n",
    "    \n",
    "    #find job url\n",
    "    job_url = each.find('a', class_='base-card__full-link')['href']\n",
    "    job_url_lst.append(job_url)\n",
    "    \n",
    "    #find company name\n",
    "    company = each.find('h4', class_='base-search-card__subtitle')\n",
    "    company = company.find('a').contents[0].strip()\n",
    "    company_lst.append(company)\n",
    "\n",
    "    # find company url\n",
    "    company_url = each.find('h4', class_='base-search-card__subtitle')\n",
    "    company_url = company_url.find('a')['href'].strip()\n",
    "    company_url_lst.append(company_url)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data in dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data={'company_name': company_lst,'job_title':job_title_lst, 'job_url': job_url_lst,  'company_url': company_url_lst}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data to csv\n",
    "df.to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
